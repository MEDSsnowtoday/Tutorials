{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import plotly.graph_objects as go\n",
    "import h5py\n",
    "from matplotlib.pyplot import *\n",
    "import imageio\n",
    "import rasterio\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sierra2001.h5',\n",
       " 'Sierra2002.h5',\n",
       " 'Sierra2003.h5',\n",
       " 'Sierra2004.h5',\n",
       " 'Sierra2005.h5',\n",
       " 'Sierra2006.h5',\n",
       " 'Sierra2007.h5',\n",
       " 'Sierra2008.h5',\n",
       " 'Sierra2009.h5',\n",
       " 'Sierra2010.h5',\n",
       " 'Sierra2011.h5',\n",
       " 'Sierra2012.h5',\n",
       " 'Sierra2013.h5',\n",
       " 'Sierra2014.h5',\n",
       " 'Sierra2015.h5',\n",
       " 'Sierra2016.h5',\n",
       " 'Sierra2017.h5',\n",
       " 'Sierra2018.h5',\n",
       " 'Sierra2019.h5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glob together all of the Snow Fraction datasets.\n",
    "snow_ds = glob.glob('*.h5')\n",
    "snow_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subdatasets of first snow fraction dataset ('Sierra2001.h5').\n",
    "datasets = gdal.Open(snow_ds[0], gdal.GA_ReadOnly).GetSubDatasets()\n",
    "\n",
    "#(sds[3] is to choose the 4th dataset in the subdirectory (i.e., snow fraction). \n",
    "#The second bracket [0] is needed to open the dataset.\n",
    "snow_data = gdal.Open(datasets[3][0])\n",
    "\n",
    "#Changes the selected dataset into an array.\n",
    "snow_data_array = snow_data.ReadAsArray()\n",
    "\n",
    "#Converts the variables to 'float' to allow us to convert NA values (255) to nans\n",
    "#We also convert 0s to nans so that when plotted on base map, only areas where data is present are shown\n",
    "snow_data_float=snow_data_array.astype('float')\n",
    "snow_data_float[snow_data_float == 255] = np.nan\n",
    "\n",
    "sf_test = np.transpose(snow_data_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcuate the Monthly Average Snow Cover for Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if we can create a range of dates starting at '2000-10-01', the first date of our first year. (Water years count from the year prior).\n",
    "month_date = pd.Series(pd.date_range(\"2000-10-01\", periods = 365, freq=\"d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list = []\n",
    "# Now that we know we can create a range of dates, we need need to be able to subset the dates by each month. \n",
    "# Let's first try with month 6. \n",
    "month = month_date[month_date.dt.month == 6]\n",
    "# Subsets our test file by range of index values for month 6  (day 243 to day 272)\n",
    "month_len = sf_test[:,:,(list(month.index))]\n",
    "# Takes the mean of each cell in x and y dimensions over the specific month. \n",
    "# Axis 2 alligns with our 3rd dimension, which are days, in this case.  \n",
    "mean = np.mean(month_len, axis = 2)\n",
    "# Appends the values to an empty list.\n",
    "month_list.append(mean)\n",
    "# Converts list to an array. \n",
    "month_array = np.array(month_list)\n",
    "# Our array has 3-dimensions (first dimension being month_mean) so we need to subset the data to be 2 dimensions. \n",
    "month_array = month_array[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure(figsize=(8, 8))\n",
    "# plt.imshow(month_array,interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Loop for Calculating Monthly Average Snow Cover for each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 12, 1841, 1334)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_list = []\n",
    "for i in range(len(snow_ds)):\n",
    "    # Get subdatasets of first snow fraction dataset ('Sierra2001.h5').\n",
    "    datasets = gdal.Open(snow_ds[i], gdal.GA_ReadOnly).GetSubDatasets()\n",
    "\n",
    "    #(sds[3] is to choose the 4th dataset in the subdirectory (i.e., snow fraction). \n",
    "    #The second bracket [0] is needed to open the dataset.\n",
    "    snow_data = gdal.Open(datasets[3][0])\n",
    "\n",
    "    #Changes the selected dataset into an array.\n",
    "    snow_data_array = snow_data.ReadAsArray()\n",
    "\n",
    "    #Converts the variables to 'float' to allow us to convert NA values (255) to nans\n",
    "    #We also convert 0s to nans so that when plotted on base map, only areas where data is present are shown\n",
    "    snow_data_float=snow_data_array.astype('float')\n",
    "    snow_data_float[snow_data_float == 255] = np.nan\n",
    "    snow_data_float[snow_data_float == 0 ] = np.nan\n",
    "\n",
    "    sf_test = np.transpose(snow_data_float)\n",
    "    \n",
    "    # Make a variable for the starting year of each water year.\n",
    "    year =  i + 2000\n",
    "    # Creates a variable for the first date in the water year. \n",
    "    start_date = pd.to_datetime(str(year) + '-10-01')\n",
    "    end_date = pd.to_datetime(str(int(year) + 1) + '-09-30')\n",
    "    # Creates a list of datetimes based on each year in our dataset.\n",
    "    year_month_date = pd.Series(pd.date_range(start = start_date, end = end_date, freq=\"d\"))\n",
    "    #Need to create an empty list to append our mean values to. \n",
    "    new_list = []\n",
    "    # For loop to calculate the mean for each month per year. \n",
    "    for j in range (1, 13):\n",
    "        # Subset the date year based on month.\n",
    "        month = year_month_date[year_month_date.dt.month == j]\n",
    "        # Subset dataset by each month.\n",
    "        month_len = sf_test[:,:,(list(month.index))]\n",
    "        # Take the mean of each month per year. \n",
    "        mean = np.mean(month_len, axis = 2)\n",
    "        # Append mean values to list per year. \n",
    "        new_list.append(mean)\n",
    "    # Append year lists to empty list. \n",
    "    time_list.append(new_list)\n",
    "# Converts list to array. \n",
    "date_array = np.array(time_list)\n",
    "np.shape(date_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Annual Mean Snow Cover for Each Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmunn\\AppData\\Local\\Temp/ipykernel_21428/3483513727.py:20: RuntimeWarning: Mean of empty slice\n",
      "  annual_snow = np.nanmean(sf_test, axis = 2)\n"
     ]
    }
   ],
   "source": [
    "year_list = []\n",
    "for i in range(len(snow_ds)):\n",
    "    # Get subdatasets of first snow fraction dataset ('Sierra2001.h5').\n",
    "    datasets = gdal.Open(snow_ds[i], gdal.GA_ReadOnly).GetSubDatasets()\n",
    "\n",
    "    #(sds[3] is to choose the 4th dataset in the subdirectory (i.e., snow fraction). \n",
    "    #The second bracket [0] is needed to open the dataset.\n",
    "    snow_data = gdal.Open(datasets[3][0])\n",
    "\n",
    "    #Changes the selected dataset into an array.\n",
    "    snow_data_array = snow_data.ReadAsArray()\n",
    "\n",
    "    #Converts the variables to 'float' to allow us to convert NA values (255) to nans\n",
    "    #We also convert 0s to nans so that when plotted on base map, only areas where data is present are shown\n",
    "    snow_data_float=snow_data_array.astype('float')\n",
    "    snow_data_float[snow_data_float == 255] = np.nan\n",
    "    snow_data_float[snow_data_float == 0 ] = np.nan\n",
    "\n",
    "    sf_test = np.transpose(snow_data_float)\n",
    "    annual_snow = np.nanmean(sf_test, axis = 2)\n",
    "    year_list.append(annual_snow)\n",
    "year_array = np.array(year_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Anomolies Per Month Per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the mean of each monthly mean\n",
    "# Note: to get true mean (weighted mean), we would need to take the mean of the cumulatice days of a single month, then divide by the number of days. \n",
    "# However, since the sample size only varies by one day every four years (leap years), this will give us near identical values. \n",
    "month_mean = np.mean(date_array, axis = 0)\n",
    "\n",
    "# Create empty list to put values in \n",
    "monthly_anom = []\n",
    "# Select year\n",
    "for i in range(len(date_array[:])):\n",
    "    year_anom = date_array[i]\n",
    "    sub_list = []\n",
    "    for j in range(len(year_anom[:])):\n",
    "        month_anom = year_anom[j] - month_mean[j]\n",
    "        sub_list.append(month_anom)\n",
    "    monthly_anom.append(sub_list)\n",
    "anom_array = np.array(monthly_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1841, 1334)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(month_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Indivdual Years and Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of list is year, month, ydim, xdim\n",
    "np.shape(date_array)\n",
    "\n",
    "# Select the first year of our dataset.\n",
    "year_one = date_array[0,:,:,:]\n",
    "# Transpose data so that the dataset is ordered as: [xdim, ydim, month]\n",
    "year_one_transposed = np.transpose(year_one, (1,2,0))\n",
    "\n",
    "#Select the first month of our first year\n",
    "year_one_month_one = year_one_transposed[:,:,0]\n",
    "\n",
    "#Select the first year of the anomaly dataset. \n",
    "year_one_anom = anom_array[0,:,:,:]\n",
    "\n",
    "year_one_month_one_anom = year_one_anom[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1841)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(year_one_month_one_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1841)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(year_one_month_one_anom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Monthly Averages to Geotiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x dimension of array\n",
    "xdim = snow_data_array.shape[1]\n",
    "# y dimension of array\n",
    "ydim = snow_data_array.shape[2]\n",
    "# Projection data of sample GeoTiff\n",
    "projection = 'PROJCS[\"Albers Conical Equal Area\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",-120],PARAMETER[\"standard_parallel_1\",34],PARAMETER[\"standard_parallel_2\",40.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",-4000000],UNIT[\"meters\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'\n",
    "# transformation data of array\n",
    "# Pull refrencing matrix from h5 file.\n",
    "ref_matrix_meta = snow_data.GetMetadata()['Grid_MODIS_GRID_500m_ReferencingMatrix'].split()\n",
    "referencing_matrix = [int(ref_matrix_meta[2]), int(ref_matrix_meta[1]), int(ref_matrix_meta[0]), int(ref_matrix_meta[5]), int(ref_matrix_meta[4]), int(ref_matrix_meta[3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Single Layer to Geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SingleGeotiff(raster_name, data, height, width, geotransform, wkt):\n",
    "    \n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "    dataset = driver.Create(\n",
    "        raster_name,\n",
    "        width,\n",
    "        height,\n",
    "        1,\n",
    "        gdal.GDT_Float32)\n",
    "\n",
    "    dataset.SetGeoTransform((\n",
    "     geotransform))\n",
    "\n",
    "    dataset.SetProjection(wkt)\n",
    "    dataset.GetRasterBand(1).WriteArray(data)\n",
    "    dataset.FlushCache()  # Write to disk.\n",
    "    return dataset, dataset.GetRasterBand(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array larger than output file, or offset off edge",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21428/4122876836.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSingleGeotiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year_one_month_one_anom.tif'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myear_one_month_one_anom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mydim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreferencing_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprojection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21428/3761160927.py\u001b[0m in \u001b[0;36mSingleGeotiff\u001b[1;34m(raster_name, data, height, width, geotransform, wkt)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetProjection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwkt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetRasterBand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWriteArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlushCache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Write to disk.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetRasterBand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\eds223\\lib\\site-packages\\osgeo\\gdal.py\u001b[0m in \u001b[0;36mWriteArray\u001b[1;34m(self, array, xoff, yoff, resample_alg, callback, callback_data)\u001b[0m\n\u001b[0;32m   3985\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mosgeo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgdal_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3987\u001b[1;33m         return gdal_array.BandWriteArray(self, array, xoff, yoff,\n\u001b[0m\u001b[0;32m   3988\u001b[0m                                           \u001b[0mresample_alg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresample_alg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3989\u001b[0m                                           \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\eds223\\lib\\site-packages\\osgeo\\gdal_array.py\u001b[0m in \u001b[0;36mBandWriteArray\u001b[1;34m(band, array, xoff, yoff, resample_alg, callback, callback_data)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mxsize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mxoff\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mband\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXSize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mysize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0myoff\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mband\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"array larger than output file, or offset off edge\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[0mdatatype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNumericTypeCodeToGDALTypeCode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array larger than output file, or offset off edge"
     ]
    }
   ],
   "source": [
    "SingleGeotiff('year_one_month_one_anom.tif', year_one_month_one_anom, ydim, xdim, referencing_matrix, projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Stacked H5 File into N Single Geotiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_one_t = np.transpose(year_one)\n",
    "path = 'tif/2001'\n",
    "for i in range(len(year_one) -1):\n",
    "    dest = ('month' + str(i + 1) + 'year_2001.tif') \n",
    "    name = os.path.join(path, dest)\n",
    "    data = year_one_transposed[:,:,i]\n",
    "    SingleGeotiff(name, data, ydim, xdim, referencing_matrix, projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tif/2001\\\\month10year_2001.tif',\n",
       " 'tif/2001\\\\month11year_2001.tif',\n",
       " 'tif/2001\\\\month1year_2001.tif',\n",
       " 'tif/2001\\\\month2year_2001.tif',\n",
       " 'tif/2001\\\\month3year_2001.tif',\n",
       " 'tif/2001\\\\month4year_2001.tif',\n",
       " 'tif/2001\\\\month5year_2001.tif',\n",
       " 'tif/2001\\\\month6year_2001.tif',\n",
       " 'tif/2001\\\\month7year_2001.tif',\n",
       " 'tif/2001\\\\month8year_2001.tif',\n",
       " 'tif/2001\\\\month9year_2001.tif']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgs = glob.glob('tif/2001/month*year*.tif')\n",
    "wgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following variables to the file you want to convert (inputfile)\n",
    "# and what you want to name your output file (outputfile)\n",
    "for i in range(len(wgs)):\n",
    "    inputfile = wgs[i]\n",
    "    outputfile = \"wgs_test\" + str(i) + \".tif\"\n",
    "    #Do not change the following line, it will reproject the geotiff file\n",
    "    ds = gdal.Warp(outputfile, inputfile, dstSRS=\"+proj=longlat +datum=WGS84 +no_defs\", dstNodata = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(year_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1841, 1334, 12)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(year_one_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'tif/2001'\n",
    "dest = 'sample'\n",
    "name = os.path.join(path, dest)\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Stacked Array to Stacked Geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedGeotiff(name, array, geo_transform, projection):\n",
    "    \n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "    DataSet = driver.Create(name, array.shape[2], array.shape[1], array.shape[0], gdal.GDT_Float32)\n",
    "    DataSet.SetGeoTransform(geo_transform)\n",
    "    DataSet.SetProjection(projection)\n",
    "    for i, image in enumerate(array, 1):\n",
    "        DataSet.GetRasterBand(i).WriteArray( image )\n",
    "    DataSet.FlushCache()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'year_one_stack.tif'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StackedGeotiff('year_one_stack.tif', year_one, referencing_matrix, projection)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5f12541159748febc435e9ce37f11542d537a74020e77854a807a32ae0dfc0f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('eds223')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
