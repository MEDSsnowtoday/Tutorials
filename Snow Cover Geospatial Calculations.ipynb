{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import plotly.graph_objects as go\n",
    "import h5py\n",
    "from matplotlib.pyplot import *\n",
    "import imageio\n",
    "import rasterio\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sierra2001.h5',\n",
       " 'Sierra2002.h5',\n",
       " 'Sierra2003.h5',\n",
       " 'Sierra2004.h5',\n",
       " 'Sierra2005.h5',\n",
       " 'Sierra2006.h5',\n",
       " 'Sierra2007.h5',\n",
       " 'Sierra2008.h5',\n",
       " 'Sierra2009.h5',\n",
       " 'Sierra2010.h5',\n",
       " 'Sierra2011.h5',\n",
       " 'Sierra2012.h5',\n",
       " 'Sierra2013.h5',\n",
       " 'Sierra2014.h5',\n",
       " 'Sierra2015.h5',\n",
       " 'Sierra2016.h5',\n",
       " 'Sierra2017.h5',\n",
       " 'Sierra2018.h5',\n",
       " 'Sierra2019.h5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glob together all of the Snow Fraction datasets.\n",
    "snow_ds = glob.glob('*.h5')\n",
    "snow_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subdatasets of first snow fraction dataset ('Sierra2001.h5').\n",
    "datasets = gdal.Open(snow_ds[0], gdal.GA_ReadOnly).GetSubDatasets()\n",
    "\n",
    "#(sds[3] is to choose the 4th dataset in the subdirectory (i.e., snow fraction). \n",
    "#The second bracket [0] is needed to open the dataset.\n",
    "snow_data = gdal.Open(datasets[3][0])\n",
    "\n",
    "#Changes the selected dataset into an array.\n",
    "snow_data_array = snow_data.ReadAsArray()\n",
    "\n",
    "#Converts the variables to 'float' to allow us to convert NA values (255) to nans\n",
    "#We also convert 0s to nans so that when plotted on base map, only areas where data is present are shown\n",
    "snow_data_float=snow_data_array.astype('float')\n",
    "snow_data_float[snow_data_float == 255] = np.nan\n",
    "\n",
    "sf_test = np.transpose(snow_data_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcuate the Monthly Average Snow Cover for Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if we can create a range of dates starting at '2000-10-01', the first date of our first year. (Water years count from the year prior).\n",
    "month_date = pd.Series(pd.date_range(\"2000-10-01\", periods = 365, freq=\"d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list = []\n",
    "# Now that we know we can create a range of dates, we need need to be able to subset the dates by each month. \n",
    "# Let's first try with month 6. \n",
    "month = month_date[month_date.dt.month == 6]\n",
    "# Subsets our test file by range of index values for month 6  (day 243 to day 272)\n",
    "month_len = sf_test[:,:,(list(month.index))]\n",
    "# Takes the mean of each cell in x and y dimensions over the specific month. \n",
    "# Axis 2 alligns with our 3rd dimension, which are days, in this case.  \n",
    "mean = np.mean(month_len, axis = 2)\n",
    "# Appends the values to an empty list.\n",
    "month_list.append(mean)\n",
    "# Converts list to an array. \n",
    "month_array = np.array(month_list)\n",
    "# Our array has 3-dimensions (first dimension being month_mean) so we need to subset the data to be 2 dimensions. \n",
    "month_array = month_array[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure(figsize=(8, 8))\n",
    "# plt.imshow(month_array,interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Loop for Calculating Monthly Average Snow Cover for each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 12, 1841, 1334)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_list = []\n",
    "for i in range(len(snow_ds)):\n",
    "    # Get subdatasets of first snow fraction dataset ('Sierra2001.h5').\n",
    "    datasets = gdal.Open(snow_ds[i], gdal.GA_ReadOnly).GetSubDatasets()\n",
    "\n",
    "    #(sds[3] is to choose the 4th dataset in the subdirectory (i.e., snow fraction). \n",
    "    #The second bracket [0] is needed to open the dataset.\n",
    "    snow_data = gdal.Open(datasets[3][0])\n",
    "\n",
    "    #Changes the selected dataset into an array.\n",
    "    snow_data_array = snow_data.ReadAsArray()\n",
    "\n",
    "    #Converts the variables to 'float' to allow us to convert NA values (255) to nans\n",
    "    #We also convert 0s to nans so that when plotted on base map, only areas where data is present are shown\n",
    "    snow_data_float=snow_data_array.astype('float')\n",
    "    snow_data_float[snow_data_float == 255] = np.nan\n",
    "    snow_data_float[snow_data_float == 0 ] = np.nan\n",
    "\n",
    "    sf_test = np.transpose(snow_data_float)\n",
    "    \n",
    "    # Make a variable for the starting year of each water year.\n",
    "    year =  i + 2000\n",
    "    # Creates a variable for the first date in the water year. \n",
    "    year_month = (str(year) + \"-10-01\")\n",
    "    # Creates a list of datetimes based on each year in our dataset.\n",
    "    year_month_date = pd.Series(pd.date_range((year_month), periods =(len(snow_data_array)), freq=\"d\"))\n",
    "    #Need to create an empty list to append our mean values to. \n",
    "    new_list = []\n",
    "    # For loop to calculate the mean for each month per year. \n",
    "    for j in range (1, 13):\n",
    "        # Subset the date year based on month.\n",
    "        month = year_month_date[year_month_date.dt.month == j]\n",
    "        # Subset dataset by each month.\n",
    "        month_len = sf_test[:,:,(list(month.index))]\n",
    "        # Take the mean of each month per year. \n",
    "        mean = np.mean(month_len, axis = 2)\n",
    "        # Append mean values to list per year. \n",
    "        new_list.append(mean)\n",
    "    # Append year lists to empty list. \n",
    "    time_list.append(new_list)\n",
    "# Converts list to array. \n",
    "date_array = np.array(time_list)\n",
    "np.shape(date_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Annual Mean Snow Cover for Each Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmunn\\AppData\\Local\\Temp\\ipykernel_22844\\3483513727.py:20: RuntimeWarning: Mean of empty slice\n",
      "  annual_snow = np.nanmean(sf_test, axis = 2)\n"
     ]
    }
   ],
   "source": [
    "year_list = []\n",
    "for i in range(len(snow_ds)):\n",
    "    # Get subdatasets of first snow fraction dataset ('Sierra2001.h5').\n",
    "    datasets = gdal.Open(snow_ds[i], gdal.GA_ReadOnly).GetSubDatasets()\n",
    "\n",
    "    #(sds[3] is to choose the 4th dataset in the subdirectory (i.e., snow fraction). \n",
    "    #The second bracket [0] is needed to open the dataset.\n",
    "    snow_data = gdal.Open(datasets[3][0])\n",
    "\n",
    "    #Changes the selected dataset into an array.\n",
    "    snow_data_array = snow_data.ReadAsArray()\n",
    "\n",
    "    #Converts the variables to 'float' to allow us to convert NA values (255) to nans\n",
    "    #We also convert 0s to nans so that when plotted on base map, only areas where data is present are shown\n",
    "    snow_data_float=snow_data_array.astype('float')\n",
    "    snow_data_float[snow_data_float == 255] = np.nan\n",
    "    snow_data_float[snow_data_float == 0 ] = np.nan\n",
    "\n",
    "    sf_test = np.transpose(snow_data_float)\n",
    "    annual_snow = np.nanmean(sf_test, axis = 2)\n",
    "    year_list.append(annual_snow)\n",
    "year_array = np.array(year_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Anomolies Per Month Per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the mean of each monthly mean\n",
    "# Note: to get true mean (weighted mean), we would need to take the mean of the cumulatice days of a single month, then divide by the number of days. \n",
    "# However, since the sample size only varies by one day every four years (leap years), this will give us near identical values. \n",
    "month_mean = np.mean(date_array, axis = 0)\n",
    "\n",
    "# Create empty list to put values in \n",
    "monthly_anom = []\n",
    "# Select year\n",
    "for i in range(len(date_array[:])):\n",
    "    year_anom = date_array[i]\n",
    "    sub_list = []\n",
    "    for j in range(len(year_anom[:])):\n",
    "        month_anom = year_anom[j] - month_mean[j]\n",
    "        sub_list.append(month_anom)\n",
    "    monthly_anom.append(sub_list)\n",
    "anom_array = np.array(monthly_anom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Indivdual Years and Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of list is year, month, ydim, xdim\n",
    "np.shape(date_array)\n",
    "\n",
    "# Select the first year of our dataset.\n",
    "year_one = date_array[0,:,:,:]\n",
    "# Transpose data so that the dataset is ordered as: [xdim, ydim, month]\n",
    "year_one_transposed = np.transpose(year_one, (1,2,0))\n",
    "\n",
    "#Select the first month of our first year\n",
    "year_one_month_one = year_one_transposed[:,:,0]\n",
    "\n",
    "#Select the first year of the anomaly dataset. \n",
    "year_one_anom = anom_array[0,:,:,:]\n",
    "\n",
    "year_one_month_one_anom = year_one_anom[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1841)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(year_one_month_one_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 12, 1334)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(year_one_month_one_anom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Monthly Averages to Geotiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x dimension of array\n",
    "xdim = snow_data_array.shape[1]\n",
    "# y dimension of array\n",
    "ydim = snow_data_array.shape[2]\n",
    "# Projection data of sample GeoTiff\n",
    "projection = 'PROJCS[\"Albers Conical Equal Area\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",-120],PARAMETER[\"standard_parallel_1\",34],PARAMETER[\"standard_parallel_2\",40.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",-4000000],UNIT[\"meters\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'\n",
    "# transformation data of array\n",
    "# Pull refrencing matrix from h5 file.\n",
    "ref_matrix_meta = snow_data.GetMetadata()['Grid_MODIS_GRID_500m_ReferencingMatrix'].split()\n",
    "referencing_matrix = [int(ref_matrix_meta[2]), int(ref_matrix_meta[1]), int(ref_matrix_meta[0]), int(ref_matrix_meta[5]), int(ref_matrix_meta[4]), int(ref_matrix_meta[3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Single Layer to Geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SingleGeotiff(raster_name, data, height, width, geotransform, wkt):\n",
    "    \n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "    dataset = driver.Create(\n",
    "        raster_name,\n",
    "        width,\n",
    "        height,\n",
    "        1,\n",
    "        gdal.GDT_Float32)\n",
    "\n",
    "    dataset.SetGeoTransform((\n",
    "     geotransform))\n",
    "\n",
    "    dataset.SetProjection(wkt)\n",
    "    dataset.GetRasterBand(1).WriteArray(data)\n",
    "    dataset.FlushCache()  # Write to disk.\n",
    "    return dataset, dataset.GetRasterBand(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<osgeo.gdal.Dataset; proxy of <Swig Object of type 'GDALDatasetShadow *' at 0x0000024F6CFF9540> >,\n",
       " <osgeo.gdal.Band; proxy of <Swig Object of type 'GDALRasterBandShadow *' at 0x00000250400CC420> >)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SingleGeotiff('year_one_month_one_anom.tif', year_one_month_one_anom, ydim, xdim, referencing_matrix, projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Stacked Array to Stacked Geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedGeotiff(name, array, geo_transform, projection):\n",
    "    \n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "    DataSet = driver.Create(name, array.shape[2], array.shape[1], array.shape[0], gdal.GDT_Float32)\n",
    "    DataSet.SetGeoTransform(geo_transform)\n",
    "    DataSet.SetProjection(projection)\n",
    "    for i, image in enumerate(array, 1):\n",
    "        DataSet.GetRasterBand(i).WriteArray( image )\n",
    "    DataSet.FlushCache()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'year_one_stack.tif'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StackedGeotiff('year_one_stack.tif', year_one, referencing_matrix, projection)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5f12541159748febc435e9ce37f11542d537a74020e77854a807a32ae0dfc0f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('eds223')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
